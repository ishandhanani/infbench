# Minimal srtctl job configuration template
# This example shows a disaggregated setup with 1 prefill node and 4 decode nodes

# Job name (used in logs directory and SLURM job name)
name: "example-job"

# Model configuration
model:
  path: "deepseek-r1"  # Alias from srtslurm.yaml or full path like "/models/deepseek-r1"
  container: "lmsysorg+sglang+v0.5.5.post2.sqsh"  # Container alias or full path
  precision: "fp8"  # fp4, fp8, fp16, bf16

# Resource allocation
resources:
  gpu_type: "gb200"  # gb200, h100 (used for GPU-specific optimizations)

  # Disaggregated mode: separate prefill and decode workers
  prefill_nodes: 1     # Number of nodes for prefill workers
  decode_nodes: 4      # Number of nodes for decode workers
  prefill_workers: 1   # Number of prefill worker processes
  decode_workers: 4    # Number of decode worker processes

  # Alternative: Aggregated mode (comment out prefill/decode and use these instead)
  # agg_nodes: 4       # Total nodes for combined workers
  # agg_workers: 4     # Number of aggregated worker processes

  gpus_per_node: 4     # GPUs per node (typically 4 for GB200, 8 for H100)

# SLURM configuration (these override srtslurm.yaml defaults)
slurm:
  account: "your-account"      # SLURM account
  partition: "batch"           # SLURM partition
  time_limit: "04:00:00"       # Wall time limit (HH:MM:SS)

# Backend configuration
backend:
  # Optional: Environment variables for prefill workers
  # These are exported before running the prefill command
  prefill_environment:
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    PYTHONUNBUFFERED: "1"
    SGLANG_ENABLE_FLASHINFER_GEMM: "1"

  # Optional: Environment variables for decode workers
  decode_environment:
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    PYTHONUNBUFFERED: "1"
    SGLANG_ENABLE_FLASHINFER_GEMM: "1"

  # SGLang configuration flags
  # All keys must use dashes (kebab-case), not underscores
  sglang_config:
    prefill:
      # Model settings
      served-model-name: "deepseek-ai/DeepSeek-R1"
      model-path: "/model/"  # Path inside container
      trust-remote-code: true

      # KV cache and memory
      kv-cache-dtype: "fp8_e4m3"
      mem-fraction-static: 0.95

      # Quantization
      quantization: "fp8"

      # Mode
      disaggregation-mode: "prefill"

      # Prefill-specific settings
      max-total-tokens: 8192
      chunked-prefill-size: 8192

      # Parallelism
      tensor-parallel-size: 4
      data-parallel-size: 1

    decode:
      # Model settings (same as prefill)
      served-model-name: "deepseek-ai/DeepSeek-R1"
      model-path: "/model/"
      trust-remote-code: true

      # KV cache and memory
      kv-cache-dtype: "fp8_e4m3"
      mem-fraction-static: 0.95

      # Quantization
      quantization: "fp8"

      # Mode
      disaggregation-mode: "decode"

      # Decode-specific settings
      chunked-prefill-size: 8192

      # Parallelism
      tensor-parallel-size: 4
      data-parallel-size: 1

# Benchmark configuration
benchmark:
  type: "sa-bench"           # sa-bench, mmlu, gpqa, or "manual" (no auto-benchmark)
  isl: 1024                  # Input sequence length
  osl: 1024                  # Output sequence length
  concurrencies: [256, 512]  # Concurrency levels to test
  req_rate: "inf"            # Request rate ("inf" for maximum throughput)

# Notes:
# - All SGLang flags are passed directly to the SGLang CLI (--flag-name value)
# - For full list of SGLang flags, see SGLang documentation
# - Use dry-run mode to preview generated commands: uv run srtctl example.yaml --dry-run
# - Logs saved to: logs/{JOB_ID}_{WORKERS}_{TIMESTAMP}/
